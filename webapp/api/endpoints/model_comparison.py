"""
Model comparison endpoint - serve comparison data for all models.

Design Pattern: Service Pattern for model comparison
Algorithm: File I/O and data transformation
Big O: O(n) where n is total calibration points across all models

Supports:
- Original 4 models (LogReg/CatBoost × Platt/Isotonic) with standard evaluation format
- Pre-game odds integration models (catboost_baseline_platt, catboost_baseline_isotonic, catboost_odds_platt, catboost_odds_isotonic) with standard evaluation format
- v2 models (catboost_baseline_platt_v2, catboost_baseline_isotonic_v2, catboost_odds_platt_v2, catboost_odds_isotonic_v2, and their no_interaction variants) with updated feature set and uses_opening_odds_baseline flag
"""

import json
from pathlib import Path
from typing import Any

from fastapi import APIRouter, HTTPException

from ..logging_config import get_logger

router = APIRouter()
logger = get_logger(__name__)


def get_model_label(filename: str) -> str:
    """Extract model label from filename."""
    if "logreg_platt" in filename:
        return "Logistic Regression + Platt"
    elif "logreg_isotonic" in filename:
        return "Logistic Regression + Isotonic"
    elif "catboost_platt" in filename and "baseline" not in filename and "odds" not in filename:
        return "CatBoost + Platt"
    elif "catboost_isotonic" in filename and "baseline" not in filename and "odds" not in filename:
        return "CatBoost + Isotonic"
    elif "catboost_baseline_platt" in filename or ("baseline" in filename and "platt" in filename):
        return "CatBoost Baseline + Platt"
    elif "catboost_baseline_isotonic" in filename or ("baseline" in filename and "isotonic" in filename):
        return "CatBoost Baseline + Isotonic"
    elif "catboost_odds_platt" in filename or ("odds" in filename and "platt" in filename):
        return "CatBoost + Opening Odds + Platt"
    elif "catboost_odds_isotonic" in filename or ("odds" in filename and "isotonic" in filename):
        return "CatBoost + Opening Odds + Isotonic"
    elif "catboost_baseline_no_interaction_platt" in filename or ("baseline" in filename and "no_interaction" in filename and "platt" in filename):
        return "CatBoost Baseline (No Interactions) + Platt"
    elif "catboost_baseline_no_interaction_isotonic" in filename or ("baseline" in filename and "no_interaction" in filename and "isotonic" in filename):
        return "CatBoost Baseline (No Interactions) + Isotonic"
    elif "catboost_odds_no_interaction_platt" in filename or ("odds" in filename and "no_interaction" in filename and "platt" in filename):
        return "CatBoost + Opening Odds (No Interactions) + Platt"
    elif "catboost_odds_no_interaction_isotonic" in filename or ("odds" in filename and "no_interaction" in filename and "isotonic" in filename):
        return "CatBoost + Opening Odds (No Interactions) + Isotonic"
    else:
        return filename


def get_model_color(model_label: str) -> str:
    """Get color for model based on label."""
    if "Logistic Regression + Platt" in model_label:
        return "#7c3aed"  # Purple
    elif "Logistic Regression + Isotonic" in model_label:
        return "#3b82f6"  # Blue
    elif "CatBoost + Platt" in model_label:
        return "#f7931a"  # Orange
    elif "CatBoost + Isotonic" in model_label and "Baseline" not in model_label and "Opening Odds" not in model_label:
        return "#10b981"  # Green
    elif "Baseline + Platt" in model_label:
        return "#ef4444"  # Red
    elif "Baseline + Isotonic" in model_label:
        return "#f59e0b"  # Amber
    elif "Opening Odds + Platt" in model_label:
        return "#8b5cf6"  # Violet
    elif "Opening Odds + Isotonic" in model_label:
        return "#ec4899"  # Pink
    elif "Baseline (No Interactions) + Platt" in model_label:
        return "#dc2626"  # Darker red
    elif "Baseline (No Interactions) + Isotonic" in model_label:
        return "#d97706"  # Darker amber
    elif "Opening Odds (No Interactions) + Platt" in model_label:
        return "#7c3aed"  # Darker violet
    elif "Opening Odds (No Interactions) + Isotonic" in model_label:
        return "#db2777"  # Darker pink
    else:
        return "#666666"  # Gray


def load_evaluation_reports(reports_dir: Path) -> list[dict[str, Any]]:
    """Load all evaluation reports (original 4 models + pre-game odds integration models + no-interaction models)."""
    # All models (standard format) - original 4 + pre-game odds integration models + no-interaction models
    model_files = [
        "winprob_eval_logreg_platt_2017-2023_calib_2023_on_2024.json",
        "winprob_eval_logreg_isotonic_2017-2023_calib_2023_on_2024.json",
        "winprob_eval_catboost_platt_2017-2023_calib_2023_on_2024.json",
        "winprob_eval_catboost_isotonic_2017-2023_calib_2023_on_2024.json",
        # Pre-game odds integration models (standard format, generated by evaluate_winprob_model.py)
        "winprob_eval_catboost_baseline_platt_calib_2023_on_2024.json",
        "winprob_eval_catboost_baseline_isotonic_calib_2023_on_2024.json",
        "winprob_eval_catboost_odds_platt_calib_2023_on_2024.json",
        "winprob_eval_catboost_odds_isotonic_calib_2023_on_2024.json",
        # No-interaction models (standard format, generated by evaluate_winprob_model.py)
        "winprob_eval_catboost_baseline_no_interaction_platt_calib_2023_on_2024.json",
        "winprob_eval_catboost_baseline_no_interaction_isotonic_calib_2023_on_2024.json",
        "winprob_eval_catboost_odds_no_interaction_platt_calib_2023_on_2024.json",
        "winprob_eval_catboost_odds_no_interaction_isotonic_calib_2023_on_2024.json",
    ]
    
    reports = []
    for filename in model_files:
        filepath = reports_dir / filename
        if not filepath.exists():
            logger.warning(f"Evaluation file not found: {filepath}, skipping")
            continue
        
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            model_label = get_model_label(filename)
            data["model_label"] = model_label
            data["model_color"] = get_model_color(model_label)
            data["filename"] = filename
            data["format"] = "standard"  # Mark as standard format
            reports.append(data)
        except Exception as e:
            logger.error(f"Error loading evaluation file {filepath}: {e}", exc_info=True)
            continue
    
    # Note: Removed fallback to time-bucketed format - all models should use standard format
    
    return reports


def convert_sprint_to_standard_format(
    metrics: dict[str, Any],
    model_name: str,
    artifact_path: str
) -> dict[str, Any]:
    """
    Convert sprint time-bucketed format to standard evaluation format.
    
    Sprint format has: brier_overall, logloss_overall, brier_2880-2400, etc.
    Standard format needs: eval.overall (logloss, brier, ece_binned, roc_auc, n) and eval.calibration_bins
    """
    # Extract overall metrics
    overall = {
        "logloss": metrics.get("logloss_overall", 0.0),
        "brier": metrics.get("brier_overall", 0.0),
        "n": metrics.get("total_snapshots", 0),
        "roc_auc": 0.0,  # Not available in sprint format
        "ece_binned": 0.0,  # Not available in sprint format
    }
    
    # Create calibration bins from time-bucketed data
    # For now, create placeholder calibration bins (would need actual predictions to compute properly)
    calibration_bins = []
    for i in range(20):  # 20 bins for calibration
        calibration_bins.append({
            "avg_p": i / 20.0 + 0.025,  # Center of bin
            "obs_rate": i / 20.0 + 0.025,  # Placeholder - would need actual data
            "n": overall["n"] // 20,  # Distribute evenly
        })
    
    model_label = get_model_label(model_name)
    
    return {
        "model_label": model_label,
        "model_color": get_model_color(model_label),
        "filename": f"sprint_{model_name}",
        "format": "sprint_converted",
        "artifact_path": artifact_path,
        "eval": {
            "overall": overall,
            "calibration_bins": calibration_bins,
        },
    }


def extract_metrics(reports: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Extract metrics from all reports."""
    metrics = []
    for report in reports:
        overall = report.get("eval", {}).get("overall", {})
        metrics.append({
            "model": report["model_label"],
            "logloss": overall.get("logloss", 0.0),
            "brier": overall.get("brier", 0.0),
            "ece": overall.get("ece_binned", 0.0),
            "auc": overall.get("roc_auc", 0.0),
            "n": overall.get("n", 0),
            "color": report["model_color"],
        })
    return metrics


def extract_calibration_points(report: dict[str, Any]) -> list[dict[str, Any]]:
    """Transform calibration_bins to calibration_points format."""
    calibration_bins = report.get("eval", {}).get("calibration_bins", [])
    points = []
    for bin_data in calibration_bins:
        points.append({
            "x": float(bin_data.get("avg_p", 0.0)),
            "y": float(bin_data.get("obs_rate", 0.0)),
            "n": int(bin_data.get("n", 0)),
        })
    return points


def find_best_models(metrics: list[dict[str, Any]]) -> dict[str, str]:
    """Find best model for each metric."""
    best = {}
    
    # Lower is better: logloss, brier, ece
    if metrics:
        best["logloss"] = min(metrics, key=lambda m: m["logloss"])["model"]
        best["brier"] = min(metrics, key=lambda m: m["brier"])["model"]
        best["ece"] = min(metrics, key=lambda m: m["ece"])["model"]
        
        # Higher is better: auc
        best["auc"] = max(metrics, key=lambda m: m["auc"])["model"]
    
    return best


@router.get("/stats/model-comparison")
def get_model_comparison() -> dict[str, Any]:
    """
    Get model comparison data for all models.
    
    Includes:
    - Original 4 models (LogReg/CatBoost × Platt/Isotonic)
    - Pre-game odds integration models (catboost_baseline_platt, catboost_baseline_isotonic, catboost_odds_platt, catboost_odds_isotonic) if available
    - v2 models (catboost_baseline_platt_v2, catboost_baseline_isotonic_v2, catboost_odds_platt_v2, catboost_odds_isotonic_v2, and their no_interaction variants) if available
    
    Returns:
        JSON with models array (metrics and calibration_points) and best_models object
    """
    try:
        # Get repository root (webapp/api/endpoints/ -> repo root)
        repo_root = Path(__file__).parent.parent.parent.parent
        reports_dir = repo_root / "data" / "models" / "evaluations"
        
        if not reports_dir.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Evaluations directory not found: {reports_dir}"
            )
        
        # Load all evaluation reports
        reports = load_evaluation_reports(reports_dir)
        
        if len(reports) == 0:
            raise HTTPException(
                status_code=404,
                detail="No evaluation reports found in data/models/evaluations/"
            )
        
        # Extract metrics and calibration points
        metrics = extract_metrics(reports)
        best_models = find_best_models(metrics)
        
        # Build response with models array
        models_data = []
        for report in reports:
            calibration_points = extract_calibration_points(report)
            overall = report.get("eval", {}).get("overall", {})
            
            models_data.append({
                "model_label": report["model_label"],
                "model_color": report["model_color"],
                "metrics": {
                    "logloss": overall.get("logloss", 0.0),
                    "brier": overall.get("brier", 0.0),
                    "ece": overall.get("ece_binned", 0.0),
                    "auc": overall.get("roc_auc", 0.0),
                    "n": overall.get("n", 0),
                },
                "calibration_points": calibration_points,
            })
        
        return {
            "models": models_data,
            "best_models": best_models,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error loading model comparison data: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Error loading model comparison data: {str(e)}"
        )

